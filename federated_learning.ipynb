{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated learning\n",
    "Una proporción importante de los modelos de IA requieren recopilar información de los usuarios para poder realizar un aprendizaje centralizado y, posteriormente, suministrar las predicciones a los usuarios que lo solicitan. Un ejemplo de esta situación son los Sistemas de Recomendación, en los que los usuarios deben aportar información tal como los productos que han comprado en una plataforma de comercio electrónico, en una plataforma de streaming de vídeo las películas que han visto y lo que les han gustado, lo mismo con canciones, etc. Esta situación presenta dos inconvenientes:\n",
    "1. Vulnera la privacidad de los usuarios, al requerirles datos personales de consumo o valoraciones de productos y servicios.\n",
    "2. Pone en peligro los datos anteriores de cientos de miles o de millones de usuarios si la empresa es hackeada con éxito.\n",
    "3. Obliga a que exista un costoso procesamiento centralizado de los millones de datos recopilados.\n",
    "\n",
    "El aprendizaje federado permite evitar los inconvenientes anteriormente mencionados. Su diseño de alto nivel se explica en la siguiente figura. En un bucle sin fin, se realizan las siguientes acciones:<br>\n",
    "\n",
    "bucle: \n",
    "\n",
    "0. El servidor envía su modelo 'global' de aprendizaje a cada uno de los usuarios que se concectan.\n",
    "1. Cada usuario entrena localmente el modelo utilizando exclusivamente sus datos actuales (últimas compras, últimas valoraciones, últimas canciones escuchadas...)\n",
    "2. Cada usuario envía al servidor su modelo recien actualizado (entrenado con sus últimos datos). \n",
    "3. El servidor actualiza un modelo 'global' con la información de los últimos modelos 'locales' enviados por los usuarios concectados. Aquí se encuentra la clave de este campo de la IA: cómo crear un modelo global a partir de _N_ modelos locales; el enfoque más sencillo, y que funciona, es simplemente promediar los pesos de todos los modelos locales; en realidad es un promedio ponderado, donde los pesos de los modelos locales más evolucionados ponderan más (por ejemplo, los modelos locales de los usuarios que han escuchado cientos de canciones desde la última vez que entrenaron su modelo ponderarán más que aquellos correspondientes a los usuarios que solo han escuchado unas pocas canciones).\n",
    "\n",
    "Nótese que cada vez que los usuarios reciben un modelo, éste está actualizado por el servidor respecto a la última versión que recibieron los usuarios. De esta manera, los _N_ modelos que recibe el servidor no parten de cero, si no que son _N_ evoluciones independientes de la última versión que había en el servidor.\n",
    "\n",
    "<br><img src=\"concepto.png\" width=500>\n",
    "<br><br>Es importante resaltar que:\n",
    "1. Los usuarios __nunca envían sus datos__, lo que envían son modelos entrenados (mejorados) con los datos que han creado en un intervalo de tiempo. Por lo tanto no se vulnera la privacidad de sus datos ni por parte de la empresa que ofrece el servicio, ni por parte de las comunicaciones entre el usaurio y el servidor. \n",
    "2. Los servidores __no reciben ni contienen ningún dato__, por lo que un hackeo de su sistema no comporta (de manera directa) un 'robo de datos'.\n",
    "3. El procesamiento que debe hacer el servidor o cloud de la empresa es muchísmimo menor que el habitual: __es mucho más rápido crear el modelo global a partir de los modelos locales que entrenar el modelo global__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mostrar el funcionamiento de un modelo federado, vamos a implementar una versión simplificada que realice una clasificación con el MNIST. En este caso, el mismo equipo va a ejecutar la parte del servidor y la de los clientes. En un caso real habría que añadir el nivel de comunicaciones servidor/cliente. El ejemplo mostrado es una versión simplificada, elaborada a partir del código en: https://towardsdatascience.com/federated-learning-a-step-by-step-implementation-in-tensorflow-aac568283399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T14:38:03.195575Z",
     "start_time": "2023-10-02T14:38:01.010390Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import np_utils\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Dropout,Flatten,Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí realizamos la carga del dataset MNIST y lo procesamos de la manera habitual para ejecutar una clasificación: pasamos las etiquetas a formato categórico y dividimos en conjuntos de entreanamiento y de testeo (10% del tamaño total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T14:38:03.462022Z",
     "start_time": "2023-10-02T14:38:03.429854Z"
    }
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# convert to categorical labels\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "#split data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, \n",
    "                                            y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código crea un diccionario con 10 clientes (tantos como categorías del MNIST), pero podría ser cualquier otro número de clientes.<br>\n",
    "{ 'clients_1': datos del cliente 1,<br>\n",
    "  'clients_2': datos del cliente 2,<br>\n",
    "  etc. }\n",
    "<br> Los datos unifican las Xs e Ys del MNIST (línea 16) convenientemente barajados (línea 17) para evitar sesgos. A cada uno de los 10 clientes les corresponde un décimo de los datos disponibles (línea 20). Esto se gestiona con facilidad creando una lista con un comprehension y usando slices (línea 21). nota: 'shard' se traduce como 'fragmento'. La línea 26 crea el diccionario descrito, y la línea 29 lo almacena en la variable 'clients'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T14:38:03.513281Z",
     "start_time": "2023-10-02T14:38:03.463776Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_clients(X_train, y_train, num_clients=10, initial='clients'):\n",
    "    ''' return: a dictionary with keys clients' names and value as \n",
    "                data shards - tuple of images and label lists.\n",
    "        args: \n",
    "            X_train: a list of numpy arrays of training images\n",
    "            y_train:a list of categorized labels for each image\n",
    "            num_client: number of fedrated members (clients)\n",
    "            initials: the clients'name prefix, e.g, clients_1 \n",
    "            \n",
    "    '''\n",
    "\n",
    "    #create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
    "\n",
    "    #randomize the data\n",
    "    data = list(zip(X_train, y_train))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    #shard data and place at each client\n",
    "    size = len(data)//num_clients\n",
    "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "\n",
    "    #number of clients must equal number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))} \n",
    "\n",
    "#create clients\n",
    "clients = create_clients(X_train, y_train, num_clients=10, initial='client')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para procesar eficientemente la información en un modelo de red neuronal, tanto los datos de shape (None,28,28,1) como las etiquetas de shape (None,10) (línea 9) se introducen, de manera unificada, en un 'Dataset' TensorFlow (línea 10). 'De nuevo' se barajan los datos en la línea 11, y se devuelven al programa llamante. En las líneas 15 a 17 se prepara las versiones tensorFlow Dataset de entrenamiento _clients_batched_, y en la línea 20 la versión TensorFlow Dataset de testeo _test_batched_. _clients_batched_ va a ser posteriormente utilizado para entrenameinto en el _fit_ del modelo, y  _test_batched_ en el código de testeo de la calidad de los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T14:38:09.782414Z",
     "start_time": "2023-10-02T14:38:03.514954Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_data(data_shard, bs=32):\n",
    "    '''Takes in a clients data shard and create a tfds object off it\n",
    "    args:\n",
    "        shard: a data, label constituting a client's data shard\n",
    "        bs:batch size\n",
    "    return:\n",
    "        tfds object'''\n",
    "    #seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    return dataset.shuffle(buffer_size=len(label)).batch(bs)\n",
    "\n",
    "\n",
    "#process and batch the training data for each client\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "#process and batch the test set  \n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí creamos un método estático (no será necesario instanciar la clase para usarlo) que nos construye un sencillo clasificador CNN para el MNIST. Al parámetro shape le tendrá que llegar como argumento el shape de MNIST: (None,28,28,1), y en 'classes' el número de categorías: 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T14:38:09.789845Z",
     "start_time": "2023-10-02T14:38:09.784841Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleMLP:\n",
    "    @staticmethod\n",
    "    def build(shape, classes):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(filters=32,kernel_size=(3,3), input_shape=shape, activation=\"relu\", padding=\"same\"))\n",
    "        model.add(MaxPooling2D())\n",
    "        model.add(Conv2D(filters=64,kernel_size=(3,3), input_shape=shape, activation=\"relu\", padding=\"same\"))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(MaxPooling2D())\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(classes, activation=\"softmax\"))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, el servidor actualizará el modelo haciendo una media ponderada de los pesos de los modelos de cada uno de los clientes. La idea es que los clientes a los que se les ha asignado más datos ponderen más en la consecución del modelo global que se crea a partir del conjunto de modelos locales. En este caso se les ha dado los mismos datos a todos (líneas 20 y 21 tres celdas hacia arriba, en la función _create_clients_). Se puede hacer el ejemplo más real cambiando esas asignaciones. <br><br>\n",
    "En la línea 2 se obtiene la relación de clientes, que se usa en la línea 3 para recorrerlos uno a uno, obtener el tamaño (la cardinalidad) del Dataset de cada uno de ellos, y sumarlo todo para hallar la cardinalidad total. En la línea 5 se halla la cardinalidad del cliente solicitado _client_name_ (parámetro de la función), y en la línea 6 se introduce en la variable _scalar_ el valor decimal que define la importancia del modelo de ese cliente (_client_name_). en las líneas 7 a 10 modificamos los pesos del modelo de este cliente para que reflejen su importancia ponderada de cara a que el servidor construya el modelo global (unificado). La ecuación de la derecha formaliza esta ponderación, donde 'K' representa a los clientes, y Fk(w) son los pesos modificados del modelo del cliente 'k' (lo que acabamos de explicar).<br><br>\n",
    "La siguiente función: _sum_scaled_weights_ se encarga de crear el modelo global a partir de los 'K' modelos locales anteriores (ecuación a la izquierda).\n",
    "<img src=\"weighting.webp\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T14:40:34.261221Z",
     "start_time": "2023-10-02T14:40:34.249414Z"
    }
   },
   "outputs": [],
   "source": [
    "def weight_scalling_factor(clients_trn_data, client_name, weight):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client]).numpy() for client in client_names])\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()\n",
    "    scalar = local_count/global_count\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)        \n",
    "    return avg_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"federated_learning_esquema.jpg\" width=70%>\n",
    "Ya estamos en disposición de simular la ejecución del servidor y la de los 'K' clientes. Las líneas 2 y 3 nos crean el modelo *global_model*, haciendo uso de la función _build_ en la que programamos el clasificador convolucional. Vamos a actualizar este modelo un número limitado de veces; en nuestro caso, *federated_loops=10*. En un caso real sería un bucle infinito, desde el punto de vista de que los usuarios siempre están enviando nuevos modelos locales al servidor y el servidor siempre les envía cada modelo global actualizado.<br><br>\n",
    "Cada vuelta del bucle (línea 7) el servidor recoge los pesos del modelo global (línea 10), que posteriormente serán enviados (en nuestro caso copiados) a cada uno de los clientes (línea 26). *scaled_local_weight_list* (línea 13) es una variable importante, ya que va a albergar la lista de todos los pesos de los modelos de todos los clientes; es decir será una lista de 'K' posiciones, donde cada una contendrá los pesos del modelo correspondiente al cliente 'K'. Cuando esa lista esté rellena, podremos unificar, en la línea 34, esos pesos (que ya fueron ponderados), y actualizar el modelo global con la información federada (línea 37).<br><br>\n",
    "En definitiva, solo nos queda rellenar esa lista *scaled_local_weight_list* con los pesos de los modelos de los *K* clientes. Para ello recorremos cada uno de los *K* clientes (línea 20), y a cada uno le construimos su propio clasificador en las líneas 21 y 22 (idéntico al de los demás clientes e idéntico al del servidor). También lo compilamos de la misma manera que los demás (línea 23). Todavía no lo podemos ejecutar, porque debemos hacerlo a partir de la última versión del modelo global (línea 26). cuando por fin lo ejecutamos (línea 28), nos aseguramos de hacerlo únicamente con los datos de ese cliente _clients_batched[client]_. Finalmente, con los pesos actualizados correspondientes al último entrenamiento del cliente procesado, obtenemos los pesos ponderados de su modelo (línea 30) y los añadimos a la lista que contiene los pesos ponderados de los *K* modelos correspondientes a los *K* clientes (línea 31).   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T14:49:11.395520Z",
     "start_time": "2023-10-02T14:40:46.713667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 14ms/step - loss: 3.0142 - accuracy: 0.7848\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.3313 - accuracy: 0.9207\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 3.3138 - accuracy: 0.7846\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.3592 - accuracy: 0.9244\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 2.8948 - accuracy: 0.7835\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.3282 - accuracy: 0.9211\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 2.9708 - accuracy: 0.7924\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.3377 - accuracy: 0.9207\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 3.0592 - accuracy: 0.7883\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.3421 - accuracy: 0.9228\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 2.8554 - accuracy: 0.7867\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.3394 - accuracy: 0.9211\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 14ms/step - loss: 2.5184 - accuracy: 0.8006\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.3602 - accuracy: 0.9241\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 2.9963 - accuracy: 0.7917\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.3412 - accuracy: 0.9193\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 3.0049 - accuracy: 0.7941\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 14ms/step - loss: 0.3587 - accuracy: 0.9206\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 14ms/step - loss: 2.9218 - accuracy: 0.7931\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.3594 - accuracy: 0.9207\n",
      "9\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.2438 - accuracy: 0.9393\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.1540 - accuracy: 0.9583\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 14ms/step - loss: 0.2723 - accuracy: 0.9320\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.1707 - accuracy: 0.9543\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.2669 - accuracy: 0.9320\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 14ms/step - loss: 0.1572 - accuracy: 0.9569\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.2673 - accuracy: 0.9315\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.1639 - accuracy: 0.9581\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.2630 - accuracy: 0.9339\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.1809 - accuracy: 0.9515\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 14ms/step - loss: 0.2440 - accuracy: 0.9383\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.1747 - accuracy: 0.9581\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 14ms/step - loss: 0.2629 - accuracy: 0.9331\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.1768 - accuracy: 0.9535\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 14ms/step - loss: 0.2691 - accuracy: 0.9335\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 14ms/step - loss: 0.1782 - accuracy: 0.9556\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.2493 - accuracy: 0.9393\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.1830 - accuracy: 0.9546\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.2584 - accuracy: 0.9326\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.1715 - accuracy: 0.9544\n",
      "8\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1688 - accuracy: 0.9578\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.1276 - accuracy: 0.9659\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 14ms/step - loss: 0.1655 - accuracy: 0.9585\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.1235 - accuracy: 0.9678\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1761 - accuracy: 0.9554\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.1233 - accuracy: 0.9670\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1719 - accuracy: 0.9557\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.1169 - accuracy: 0.9689\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1831 - accuracy: 0.9519\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.1301 - accuracy: 0.9652\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1517 - accuracy: 0.9596\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.1033 - accuracy: 0.9711\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 14ms/step - loss: 0.1586 - accuracy: 0.9583\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.1165 - accuracy: 0.9676\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1691 - accuracy: 0.9572\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.1266 - accuracy: 0.9659\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1867 - accuracy: 0.9548\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.1359 - accuracy: 0.9639\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1658 - accuracy: 0.9583\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.1158 - accuracy: 0.9657\n",
      "7\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 14ms/step - loss: 0.1343 - accuracy: 0.9691\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 14ms/step - loss: 0.0941 - accuracy: 0.9731\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1425 - accuracy: 0.9667\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0903 - accuracy: 0.9741\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1429 - accuracy: 0.9620\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0968 - accuracy: 0.9726\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1339 - accuracy: 0.9657\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 14ms/step - loss: 0.0935 - accuracy: 0.9754\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1357 - accuracy: 0.9644\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.1030 - accuracy: 0.9720\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1251 - accuracy: 0.9644\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0857 - accuracy: 0.9750\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1364 - accuracy: 0.9654\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0971 - accuracy: 0.9757\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1361 - accuracy: 0.9626\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.1002 - accuracy: 0.9698\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1405 - accuracy: 0.9635\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0983 - accuracy: 0.9754\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1348 - accuracy: 0.9630\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 14ms/step - loss: 0.0956 - accuracy: 0.9726\n",
      "6\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 14ms/step - loss: 0.1169 - accuracy: 0.9689\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.1023 - accuracy: 0.9726\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1093 - accuracy: 0.9731\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0805 - accuracy: 0.9791\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1249 - accuracy: 0.9665\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0744 - accuracy: 0.9785\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1264 - accuracy: 0.9672\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0919 - accuracy: 0.9774\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1039 - accuracy: 0.9724\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0850 - accuracy: 0.9809\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1127 - accuracy: 0.9724\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0822 - accuracy: 0.9756\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1049 - accuracy: 0.9746\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0709 - accuracy: 0.9793\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1127 - accuracy: 0.9693\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0932 - accuracy: 0.9731\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0882 - accuracy: 0.9744\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0796 - accuracy: 0.9780\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1108 - accuracy: 0.9691\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0852 - accuracy: 0.9770\n",
      "5\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0942 - accuracy: 0.9763\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0702 - accuracy: 0.9798\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0972 - accuracy: 0.9752\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0727 - accuracy: 0.9820\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1082 - accuracy: 0.9715\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0777 - accuracy: 0.9793\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 14ms/step - loss: 0.0929 - accuracy: 0.9756\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0804 - accuracy: 0.9783\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1026 - accuracy: 0.9731\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0647 - accuracy: 0.9837\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1106 - accuracy: 0.9730\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0707 - accuracy: 0.9796\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1054 - accuracy: 0.9733\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0831 - accuracy: 0.9793\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0926 - accuracy: 0.9750\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0734 - accuracy: 0.9796\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0756 - accuracy: 0.9804\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0700 - accuracy: 0.9819\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1060 - accuracy: 0.9744\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0730 - accuracy: 0.9793\n",
      "4\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0789 - accuracy: 0.9791\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0500 - accuracy: 0.9843\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.1083 - accuracy: 0.9735\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0772 - accuracy: 0.9798\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 14ms/step - loss: 0.0908 - accuracy: 0.9765\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0638 - accuracy: 0.9820\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0937 - accuracy: 0.9756\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0684 - accuracy: 0.9806\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 14ms/step - loss: 0.1063 - accuracy: 0.9746\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0726 - accuracy: 0.9809\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0975 - accuracy: 0.9756\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0611 - accuracy: 0.9835\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0825 - accuracy: 0.9780\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0676 - accuracy: 0.9820\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0782 - accuracy: 0.9783\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0668 - accuracy: 0.9833\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0841 - accuracy: 0.9796\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0633 - accuracy: 0.9824\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0900 - accuracy: 0.9756\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0640 - accuracy: 0.9813\n",
      "3\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0745 - accuracy: 0.9806\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0539 - accuracy: 0.9846\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0729 - accuracy: 0.9804\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0611 - accuracy: 0.9811\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0920 - accuracy: 0.9772\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0656 - accuracy: 0.9830\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0837 - accuracy: 0.9778\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0585 - accuracy: 0.9815\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0792 - accuracy: 0.9798\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0700 - accuracy: 0.9828\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0822 - accuracy: 0.9783\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0612 - accuracy: 0.9830\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0782 - accuracy: 0.9789\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0571 - accuracy: 0.9826\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0812 - accuracy: 0.9793\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0604 - accuracy: 0.9848\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0981 - accuracy: 0.9756\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0646 - accuracy: 0.9833\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0980 - accuracy: 0.9731\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0622 - accuracy: 0.9830\n",
      "2\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0870 - accuracy: 0.9783\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0582 - accuracy: 0.9856\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0875 - accuracy: 0.9759\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0700 - accuracy: 0.9822\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0689 - accuracy: 0.9796\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0440 - accuracy: 0.9869\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0854 - accuracy: 0.9817\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0594 - accuracy: 0.9811\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0788 - accuracy: 0.9793\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0577 - accuracy: 0.9844\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 14ms/step - loss: 0.0785 - accuracy: 0.9783\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0612 - accuracy: 0.9822\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0828 - accuracy: 0.9798\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0626 - accuracy: 0.9807\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0847 - accuracy: 0.9761\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0678 - accuracy: 0.9796\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0778 - accuracy: 0.9785\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0644 - accuracy: 0.9841\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0764 - accuracy: 0.9826\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0585 - accuracy: 0.9863\n",
      "1\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0822 - accuracy: 0.9793\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0510 - accuracy: 0.9869\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0860 - accuracy: 0.9787\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 14ms/step - loss: 0.0545 - accuracy: 0.9857\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0651 - accuracy: 0.9809\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0582 - accuracy: 0.9848\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0785 - accuracy: 0.9804\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0613 - accuracy: 0.9830\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0564 - accuracy: 0.9846\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0417 - accuracy: 0.9887\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0876 - accuracy: 0.9774\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0638 - accuracy: 0.9830\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0818 - accuracy: 0.9804\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0535 - accuracy: 0.9865\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0756 - accuracy: 0.9793\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 14ms/step - loss: 0.0528 - accuracy: 0.9843\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0743 - accuracy: 0.9811\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0443 - accuracy: 0.9854\n",
      "Epoch 1/2\n",
      "169/169 [==============================] - 3s 13ms/step - loss: 0.0691 - accuracy: 0.9824\n",
      "Epoch 2/2\n",
      "169/169 [==============================] - 2s 13ms/step - loss: 0.0628 - accuracy: 0.9848\n"
     ]
    }
   ],
   "source": [
    "#initialize global model\n",
    "smlp_global = SimpleMLP()\n",
    "global_model = smlp_global.build((28,28,1), 10)\n",
    "\n",
    "federated_loops = 10\n",
    "#commence global training loop\n",
    "for current_loop in range(federated_loops):\n",
    "    print(federated_loops-current_loop)            \n",
    "    # get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    #randomize client data - using keys\n",
    "    client_names= list(clients_batched.keys())\n",
    "    random.shuffle(client_names)\n",
    "    \n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:\n",
    "        smlp_local = SimpleMLP()\n",
    "        local_model = smlp_local.build((28,28,1), 10)\n",
    "        local_model.compile(loss='categorical_crossentropy', metrics=['accuracy'] )\n",
    "        \n",
    "        #set local model weight to the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "        #fit local model with client's data\n",
    "        local_model.fit(clients_batched[client], epochs=2, verbose=1, batch_size=512)\n",
    "        #scale the model weights and add to list\n",
    "        scaled_weights = weight_scalling_factor(clients_batched, client,local_model.get_weights())\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "        \n",
    "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    \n",
    "    #update global model \n",
    "    global_model.set_weights(average_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es el momento de comparar las calidades obtenidas con el modelo federado y el modelo global. Antes de nada, indicar que este ejemplo no es representativo del sesgo que se dará en situaciones reales; aquí todos los clientes tienen una porción aleatoria y de igual tamaño de las imágenes del MNIST. En casos reales, cada usuario aporta modelos 'sesgados' en el sentido de que sus datos tendrán distribuciones de probabilidad algo diferentes a las de otros usuarios. Por ejemplo, un usuario habrá aportado pocos datos, mientras que otro habrá contribuido con muchos, uno habrá adquirido bastantes productos para bebé, mientras que la mayoría no, etc. La manera de unificar los diferentes modelos locales en un modelo global es actualmente motivo de investigación.<br><br>\n",
    "Para testear el modelo federado, en las líneas 10 y 11 se proporcionan los datos *X_test* y etiquetas *y_test* de testeo preparados al comienzo del notebook, y se llama a la función *test_model* que hace el feedforward (*predict*) de los datos (*X_test*), obteniendo las predicciones que aporta el clasificador global (*y_pred* en la línea 3). Estas predicciones se comparan, en la línea 4, con las etiquetas (*Y_test) utilizando *CategoricalCrossentropy* (línea 2), para hallar el loss. También se comparan para hallar el accuracy (línea 5). Ambas medidas se imprimen en la línea 6.<br><br>\n",
    "Para testear el modelo global, nos 'cargamos' el modelo federado; para ello, entre las líneas 14 y 20 definimos de nuevo el *global_model* y lo entrenamos desde el principio con los datos del MNIST (nada de pesos ponderados de cada cliente). En las líneas 23 y 24 testeamos sus resultados.<br><br>\n",
    "Como se puede observar, en este caso tan sencillo, equilibrado, y no sesgado, los resultados federados y tradicionales tienen la misma calidad. Lo importante es que hemos comprobado que __la implementación \"federated learning\" funciona__, y a partir de aquí se puede adaptar a datos y situaciones que sean más complejos y reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T14:55:09.059830Z",
     "start_time": "2023-10-02T14:54:16.079919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 1s 4ms/step\n",
      "188/188 [==============================] - 1s 4ms/step\n",
      "comm_round: 10 | global_acc: 98.850% | global_loss: 1.474400281906128\n",
      "Epoch 1/10\n",
      "169/169 [==============================] - 5s 29ms/step - loss: 2.7051 - accuracy: 0.8519\n",
      "Epoch 2/10\n",
      "169/169 [==============================] - 5s 28ms/step - loss: 0.1202 - accuracy: 0.9649\n",
      "Epoch 3/10\n",
      "169/169 [==============================] - 5s 28ms/step - loss: 0.0758 - accuracy: 0.9781\n",
      "Epoch 4/10\n",
      "169/169 [==============================] - 5s 28ms/step - loss: 0.0579 - accuracy: 0.9826\n",
      "Epoch 5/10\n",
      "169/169 [==============================] - 5s 28ms/step - loss: 0.0492 - accuracy: 0.9852\n",
      "Epoch 6/10\n",
      "169/169 [==============================] - 5s 28ms/step - loss: 0.0434 - accuracy: 0.9873\n",
      "Epoch 7/10\n",
      "169/169 [==============================] - 5s 29ms/step - loss: 0.0389 - accuracy: 0.9882\n",
      "Epoch 8/10\n",
      "169/169 [==============================] - 5s 28ms/step - loss: 0.0365 - accuracy: 0.9896\n",
      "Epoch 9/10\n",
      "169/169 [==============================] - 5s 28ms/step - loss: 0.0308 - accuracy: 0.9907\n",
      "Epoch 10/10\n",
      "169/169 [==============================] - 5s 28ms/step - loss: 0.0307 - accuracy: 0.9906\n",
      "188/188 [==============================] - 1s 5ms/step\n",
      "188/188 [==============================] - 1s 3ms/step\n",
      "comm_round: 1 | global_acc: 98.650% | global_loss: 1.4762946367263794\n"
     ]
    }
   ],
   "source": [
    "def test_model(X_test, Y_test,  model, federated_loops):\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    y_pred = model.predict(X_test)\n",
    "    loss = cce(Y_test, y_pred)\n",
    "    acc = accuracy_score(tf.argmax(y_pred, axis=1), tf.argmax(Y_test, axis=1))\n",
    "    print('federated_loops: {} | global_acc: {:.3%} | global_loss: {}'.format(federated_loops, acc, loss))\n",
    "    return acc, loss\n",
    "\n",
    "#test global model (federated learning) and print out metrics after each communications round\n",
    "for(X_test, Y_test) in test_batched:\n",
    "    global_acc, global_loss = test_model(X_test, Y_test, global_model, federated_loops)\n",
    "\n",
    "# Test traditional MNIST classification    \n",
    "global_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(y_train)).batch(320)\n",
    "smlp = SimpleMLP()\n",
    "global_model = smlp.build((28,28,1), 10) \n",
    "\n",
    "# compile & fit the global training data to model\n",
    "global_model.compile(loss='categorical_crossentropy', metrics=['accuracy'] )\n",
    "global_model.fit(global_dataset, epochs=10, verbose=1, batch_size=512)\n",
    "\n",
    "#test the federated model and print out metrics\n",
    "for(X_test, Y_test) in test_batched:\n",
    "        acc, loss = test_model(X_test, Y_test, global_model, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
